//package Model
//
//import java.sql.Timestamp
//
//import base._
//import org.apache.spark.mllib.linalg.Vectors
//import org.apache.spark.mllib.regression.LabeledPoint
//import org.apache.spark.mllib.tree.DecisionTree
//import org.apache.spark.rdd.RDD
//import org.apache.spark.{SparkConf, SparkContext}
//object DT {
//  val DTTimeDivision = Timestamp.valueOf("2014-12-17 0:0:0")
//  val t1 = Timestamp.valueOf("2014-12-10 0:0:0")
//  val t2 = Timestamp.valueOf("2014-12-14 0:0:0")
//  val t3 = Timestamp.valueOf("2014-12-17 0:0:0")
//  case class TFeature(rating: Double,activeDegree: Double,traffic:Int,purchase: Int,conversion: Double,lastLogin:Long,firstAccess:Long, lastAccess:Long,label:Long){
//    def toDense()={
//      Vectors.dense(Array(
//        rating.toDouble,
//        activeDegree.toDouble,
//        traffic.toDouble,
//        purchase.toDouble,
//        conversion.toDouble,
//        lastLogin.toDouble,
//        firstAccess.toDouble,
//        lastAccess.toDouble)
//      )
//    }
//  }
//  def main(args: Array[String]) {
//
//    val conf = new SparkConf().setAppName("DT")
//    val sc = new SparkContext(conf)
//    val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
//    val alsAns = ALSTrans.getRating(sc)
//    val testSet = alsAns._2
//    val inputData = alsAns._1
//    val flatData = inputData.flatMap(x =>x._2._2.map(y => (x._1,x._2._1,y)))
//    val sp = splitByDate(flatData)
//    val d2List = sp._2.filter(x=> x._2._2.exists(x => x.behaviorType==4)).map(x => x._1).collect()
//
//    val handledInputData = getFeature(sp._1,t2,d2List)
//    val trainingData = handledInputData.map(x => {
//      (x._1,LabeledPoint(x._2.label,x._2.toDense()))
//    })
//
//    val numClasses = 2
//    val categoricalFeaturesInfo = Map[Int, Int]()
//    val impurity = "gini"
//    val maxDepth = 5
//    val maxBins = 32
//
//    val model = DecisionTree.trainClassifier(trainingData.map(x => x._2), numClasses, categoricalFeaturesInfo,
//      impurity, maxDepth, maxBins)
//
//    val testd2 = getFeature(sp._1,t3,Array()).map(x => {
//      (x._1,LabeledPoint(x._2.label,x._2.toDense()))
//    })
//    // Evaluate model on test instances and compute test error
//    val prediction = testd2.map { point =>
//      val predict = model.predict(point._2.features)
//      (point._1, predict,point._2.features.apply(0))
//    }.filter(x => x._2 ==1)
//    .map(x => ((x._1._1.toInt,x._1._2.toInt),x._2))
//
//    val dt = prediction.map(x => (x._1._1,x._1._2))
//
//    println("preSum:"+prediction.count())
//
//    val EVA = ALSTrans.evaluate(
//      dt, testSet.map(x => (x.userId.toInt, x.itemId.toInt)).distinct())
//    println("recall:" + EVA._1)
//    println("precesion:" + EVA._2)
//  }
//  def splitByDate(rdd: RDD[((Int,Int),Double,Data)]) ={
//    (
//      rdd.filter(x => x._3.time.before(t2)).map(x => (x._1,(x._2,List(x._3)))).reduceByKey((x,y)=>(x._1,x._2:::y._2)),
//      rdd.filter(x => x._3.time.after(t2)).map(x => (x._1,(x._2,List(x._3)))).reduceByKey((x,y)=>(x._1,x._2:::y._2))
//      )
//  }
//  def getFeature(rdd:RDD[((Int,Int),(Double,List[UIData]))],timeDiv: Timestamp,labelList:Array[(Int,Int)])={
//    rdd.map(x => {
//      val uiList = x._2._2.toList
//      val purchase = uiList.count(x => x.behaviorType ==4)
//      val traffic = uiList.size
//      val firstAccess = timeDiv.getTime - uiList.map(x => x.time.getTime).reduce((x,y) => Math.min(x,y))
//      val lastAccess = timeDiv.getTime - uiList.map(x => x.time.getTime).reduce((x,y) => Math.max(x,y))
//      val conversion = purchase.toDouble/traffic.toDouble
//      /**
//       * 17号的数据作为标签
//       */
//      val label = if(labelList.contains(x._1) ==true)1 else 0
//      (x._1, TFeature(
//        rating = x._2._1,
//        activeDegree = 1,
//        traffic = traffic,
//        purchase = purchase,
//        conversion = conversion,
//        lastLogin = 1,
//        firstAccess = firstAccess,
//        lastAccess = lastAccess,
//        label = label
//      ))
//    })
//  }
//}